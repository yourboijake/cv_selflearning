There are several significant papers that explore sequence-to-sequence approaches with attention mechanisms for OCR on handwritten text. Here are some of the most influential works in this area:

1. **"Attention-Based Models for Text Recognition" by Bahdanau et al. (2016)**
   - Pioneered attention mechanisms for OCR
   - Improved recognition of connected handwritten text by allowing the model to focus on different parts of the image while generating each character

2. **"CRNN: Convolutional Recurrent Neural Network for Scene Text Recognition" by Shi et al. (2016)**
   - Combined CNNs with bidirectional LSTMs
   - While not explicitly using attention, this paper laid crucial groundwork for end-to-end trainable handwritten text recognition

3. **"Focusing Attention: Towards Accurate Text Recognition in Natural Images" by Cheng et al. (2017)**
   - Introduced the FAN (Focusing Attention Network) for text recognition
   - Used attention to handle text with arbitrary shapes and orientations

4. **"Handwriting Recognition with Large Multidimensional Long Short-Term Memory Recurrent Neural Networks" by Voigtlaender et al. (2016)**
   - Applied MDLSTM with attention for handwritten text recognition
   - Achieved state-of-the-art results on several handwriting datasets

5. **"Gated Attention Mechanisms for Recognition of Cross-Domain Activities" by Hu et al. (2018)**
   - Introduced gated attention that helps filter irrelevant information
   - Particularly useful for noisy handwritten documents

6. **"Show, Attend and Read: A Simple and Strong Baseline for Irregular Text Recognition" by Li et al. (2019)**
   - Combined 2D attention with a CNN-LSTM architecture
   - Achieved impressive results on curved and irregular text recognition

7. **"What Is Wrong With Scene Text Recognition Model Comparisons? Dataset and Model Analysis" by Baek et al. (2019)**
   - Comprehensive analysis of attention-based models for text recognition
   - Provides insights into which attention mechanisms work best for different types of text

8. **"Transformer-based Scene Text Recognition with Non-autoregressive Enhancement" by Mou et al. (2021)**
   - Uses transformer architecture with specialized attention mechanisms
   - Improves speed while maintaining accuracy for text recognition

9. **"SCATTER: Selective Context Attentional Scene Text Recognizer" by Litman et al. (2020)**
   - Uses selective contextual attention that adapts to different recognition scenarios
   - Particularly effective for handwritten text with varying styles

10. **"SEED: Semantics Enhanced Encoder-Decoder Framework for Scene Text Recognition" by Qiao et al. (2020)**
    - Introduces semantic enhancement in the attention mechanism
    - Improves recognition of handwritten text with contextual information

If you're specifically interested in historical handwritten document recognition, you might also want to look at:

- **"READ: Recent Developments in Handwritten Historical Document Recognition" by SÃ¡nchez et al. (2019)**
  - Reviews state-of-the-art attention-based approaches for historical manuscripts
  - Discusses challenges specific to historical handwritten text

These papers represent the evolution of attention-based approaches for OCR, particularly for handwritten text recognition, moving from simple attention mechanisms to more sophisticated transformer-based architectures.