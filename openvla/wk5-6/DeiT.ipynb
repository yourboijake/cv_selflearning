{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "28142fec-936f-4c6c-9df0-3b555cc9a166",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use conv net to train a vision transformer on CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9765dc0d-f0be-44cf-916f-61ae047477ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('xpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b6dd30d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act1): ReLU(inplace=True)\n",
       "  (maxpool): Identity()\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_teacher_model(device):\n",
    "  '''\n",
    "  get resnet18 teacher model from timm, with pretrained weights\n",
    "  '''\n",
    "  import timm\n",
    "  resnet18 = timm.create_model(\"resnet18_cifar10\", pretrained=True)\n",
    "  #don't want to accidentally update params during training\n",
    "  for p in resnet18.parameters():\n",
    "    p.requires_grad = False\n",
    "  resnet18.to(device)\n",
    "  return resnet18\n",
    "\n",
    "get_teacher_model(torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c0eefbd-fb31-4e7c-a0bd-51c284afeb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/Desktop/programming/learning/ml/cv_selflearning/.venv/lib/python3.12/site-packages/outdated/__init__.py:36: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import parse_version\n",
      "/home/jacob/Desktop/programming/learning/ml/cv_selflearning/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m resnet18.parameters():\n\u001b[32m      7\u001b[39m   p.requires_grad = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m resnet18.to(\u001b[43mdevice\u001b[49m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mmodel running on \u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28mnext\u001b[39m(resnet18.parameters()).device)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mnumber of parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m([p.numel()\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mp\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mresnet18.parameters()])\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m, )\n",
      "\u001b[31mNameError\u001b[39m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "import detectors\n",
    "import timm\n",
    "\n",
    "resnet18 = timm.create_model(\"resnet18_cifar10\", pretrained=True)\n",
    "#don't want to do any retraining on teacher model\n",
    "for p in resnet18.parameters():\n",
    "  p.requires_grad = False\n",
    "resnet18.to(device)\n",
    "print('model running on ', next(resnet18.parameters()).device)\n",
    "print(f'number of parameters: {sum([p.numel() for p in resnet18.parameters()]):,}', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa66cf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_student_model():\n",
    "  \"\"\"Create ViT model for CIFAR-10\"\"\"\n",
    "  model = timm.create_model('vit_tiny_patch16_224', pretrained=False, num_classes=10)\n",
    "  #model.to(device)\n",
    "  return model\n",
    "\n",
    "student_model = get_student_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "458552be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (patch_drop): Identity()\n",
       "  (norm_pre): Identity()\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "  (fc_norm): Identity()\n",
       "  (head_drop): Dropout(p=0.0, inplace=False)\n",
       "  (head): Linear(in_features=192, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d446da95-a649-430d-9dbc-fc7b0905e334",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataset with pytorch datset and dataloaders\n",
    "transform = torchvision.transforms.ToTensor()\n",
    "TRAIN_BATCH_SIZE = 128\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', \n",
    "    train=True,\n",
    "    download=True, \n",
    "    transform=transform,\n",
    ")\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, \n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True, \n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', \n",
    "    train=False,\n",
    "    download=True, \n",
    "    transform=transform,\n",
    ")\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, \n",
    "    batch_size=200,\n",
    "    shuffle=False, \n",
    "    num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a68fa04b-cef4-435b-bb66-0eb0b1eeea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create class map: index to output class name\n",
    "class_names = {\n",
    "    0: 'airplane',\n",
    "    1: 'automobile',\n",
    "    2: 'bird',\n",
    "    3: 'cat',\n",
    "    4: 'deer',\n",
    "    5: 'dog',\n",
    "    6: 'frog',\n",
    "    7: 'horse',\n",
    "    8: 'ship',\n",
    "    9: 'truck'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "46506763-9fa1-433f-80e7-a86611187c3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'truck')"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = 1\n",
    "img, label = trainset.__getitem__(idx)\n",
    "plt.imshow(img.transpose(0, 2).transpose(0, 1))\n",
    "plt.title(class_names[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "24365361",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy of classification\n",
    "def eval_model(model, testloader, device):\n",
    "  acc_list = []\n",
    "  denom = 0\n",
    "  for i, data in enumerate(testloader, 0):\n",
    "    inputs, labels = data\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    out = model(inputs)\n",
    "    preds = out.argmax(dim=-1)\n",
    "    acc = preds.eq(labels).sum()\n",
    "    denom += inputs.shape[0]\n",
    "    acc_list.append(acc)\n",
    "  return sum(acc_list) / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f2682271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9323, device='xpu:0')"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show accuracy of timm resnet18 model\n",
    "\n",
    "eval_model(resnet18, testloader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f558cbec-e390-4fbc-a4b0-cb68303e1286",
   "metadata": {},
   "source": [
    "### Initialize ViT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "442b1137-cf96-4d09-a153-fdcd505e5304",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, embed_dim, dropout, upsample_ratio=4):\n",
    "        super().__init__()\n",
    "        self.nn = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.Linear(embed_dim, embed_dim*upsample_ratio),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim*upsample_ratio, embed_dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.nn(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7ca00cfe-dfca-45c3-b5fa-22d6e93b3ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSA(nn.Module):\n",
    "  def __init__(self, embed_dim, num_heads, dropout, use_mask):\n",
    "    super().__init__()\n",
    "    self.ln = nn.LayerNorm(embed_dim)\n",
    "    self.num_heads = num_heads\n",
    "    self.d_k = embed_dim // num_heads\n",
    "    self.Wq = nn.Linear(embed_dim, embed_dim)\n",
    "    self.Wk = nn.Linear(embed_dim, embed_dim)\n",
    "    self.Wv = nn.Linear(embed_dim, embed_dim)\n",
    "    self.msa = nn.MultiheadAttention(\n",
    "      embed_dim=embed_dim,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "      batch_first=True\n",
    "    )\n",
    "    self.use_mask = use_mask\n",
    "\n",
    "  def forward(self, X):\n",
    "    X_ln = self.ln(X)\n",
    "    batch_size, seq_len = X.shape[0], X.shape[1]\n",
    "    Q = self.Wq(X_ln)\n",
    "    K = self.Wk(X_ln)\n",
    "    V = self.Wv(X_ln)\n",
    "    if self.use_mask:\n",
    "      mask = torch.tril(torch.ones((seq_len, seq_len), dtype=torch.bool), diagonal=-1)\n",
    "      if next(self.Wq.parameters()).device.type == 'xpu':\n",
    "        mask = mask.to(device)      \n",
    "      X_msa = self.msa(Q, K, V, attn_mask=mask)\n",
    "    else:\n",
    "      X_msa = self.msa(Q, K, V)\n",
    "    return X_msa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "18cbe0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "  def __init__(self, embed_dim, num_heads, dropout, use_mask):\n",
    "    super().__init__()\n",
    "    self.msa = MSA(\n",
    "      embed_dim=embed_dim, \n",
    "      num_heads=num_heads, \n",
    "      dropout=dropout, \n",
    "      use_mask=use_mask\n",
    "    )\n",
    "    self.mlp = MLP(\n",
    "      embed_dim=embed_dim,\n",
    "      dropout=dropout,\n",
    "    )\n",
    "  \n",
    "  def forward(self, X):\n",
    "    X_msa, msa_weights = self.msa(X)\n",
    "    X = X + X_msa\n",
    "    X_mlp = self.mlp(X)\n",
    "    X = X + X_mlp\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e0e220ae-17d1-4de4-bea4-5ae0b0590880",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "  def __init__(self, embed_dim, unfolded_patch_dim, max_seq_len):\n",
    "    super().__init__()\n",
    "    self.embed_dim = embed_dim\n",
    "    self.max_seq_len = max_seq_len\n",
    "    self.x_class = nn.Parameter(torch.randn(1, 1, embed_dim), requires_grad=True)\n",
    "    self.embedding = nn.Linear(in_features=unfolded_patch_dim, out_features=embed_dim, bias=False)\n",
    "    self.pos_embedding = nn.Embedding(num_embeddings=max_seq_len, embedding_dim=embed_dim)\n",
    "\n",
    "  def forward(self, X):\n",
    "    batch_size, seq_len = X.shape[0], X.shape[1] + 1\n",
    "    X_e = self.embedding(X)\n",
    "    x_class_e = self.x_class.expand(batch_size, 1, self.embed_dim)\n",
    "    X_e = torch.concat(\n",
    "      [x_class_e, X_e],\n",
    "      dim=1\n",
    "    )\n",
    "    seq = torch.arange(seq_len)\n",
    "    if next(self.pos_embedding.parameters()).device.type == 'xpu':\n",
    "      seq = seq.to(device)\n",
    "    X_pe = self.pos_embedding(seq).unsqueeze(0)\n",
    "    return X_e + X_pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9589695f-3dd4-4da7-b84b-5f40e60e9211",
   "metadata": {},
   "outputs": [],
   "source": [
    "#img_hw is tuple of (height, width), and so is patch_size\n",
    "class ViT(nn.Module):\n",
    "  def __init__(self, embed_dim, num_classes, patch_size, img_hw, num_heads, dropout=0.0, use_mask=True):\n",
    "    super().__init__()\n",
    "    self.patch_size = patch_size\n",
    "    self.n = (img_hw[0] * img_hw[1]) // (self.patch_size ** 2)\n",
    "    self.unfolded_patch_dim = self.patch_size ** 2 * 3 #patch size num of pixels * 3 color channels\n",
    "    self.embeddings = Embeddings(\n",
    "      embed_dim=embed_dim, \n",
    "      unfolded_patch_dim=self.unfolded_patch_dim, \n",
    "      max_seq_len=self.n + 1\n",
    "    )\n",
    "    self.attention_blocks = nn.Sequential(\n",
    "      AttentionBlock(embed_dim=embed_dim, num_heads=num_heads, dropout=dropout, use_mask=use_mask),\n",
    "      AttentionBlock(embed_dim=embed_dim, num_heads=num_heads, dropout=dropout, use_mask=use_mask),\n",
    "      AttentionBlock(embed_dim=embed_dim, num_heads=num_heads, dropout=dropout, use_mask=use_mask),\n",
    "      AttentionBlock(embed_dim=embed_dim, num_heads=num_heads, dropout=dropout, use_mask=use_mask),\n",
    "      AttentionBlock(embed_dim=embed_dim, num_heads=num_heads, dropout=dropout, use_mask=use_mask),\n",
    "      AttentionBlock(embed_dim=embed_dim, num_heads=num_heads, dropout=dropout, use_mask=use_mask),\n",
    "      AttentionBlock(embed_dim=embed_dim, num_heads=num_heads, dropout=dropout, use_mask=use_mask),\n",
    "      AttentionBlock(embed_dim=embed_dim, num_heads=num_heads, dropout=dropout, use_mask=use_mask),\n",
    "    )\n",
    "    self.ln = nn.LayerNorm(embed_dim)\n",
    "    self.ll_out = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "  def forward(self, X):\n",
    "    X = F.unfold(\n",
    "      X, \n",
    "      kernel_size=self.patch_size, \n",
    "      stride=self.patch_size)\\\n",
    "      .transpose(-1, -2) #unfold img into patches\n",
    "    X = self.embeddings(X)\n",
    "    X = self.attention_blocks(X)\n",
    "    x_class = X[:, 0, :]\n",
    "    out = self.ll_out(self.ln(x_class))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f40a77-a975-42c2-9dcc-29dd12de8e0c",
   "metadata": {},
   "source": [
    "### Train ViT using normal approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "dc3f0712-79ba-4e9f-b885-214dfe5ad503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of params: 1,998,602\n"
     ]
    }
   ],
   "source": [
    "vit_normal = ViT(\n",
    "  embed_dim=128,\n",
    "  num_classes=10,\n",
    "  patch_size=4,\n",
    "  img_hw=(32, 32),\n",
    "  num_heads=8,\n",
    "  dropout=0.2,\n",
    ")\n",
    "vit_normal.to(device)\n",
    "\n",
    "print(f'number of params: {sum([p.numel() for p in vit_normal.parameters()]):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b7309825-5a90-4ee8-8c90-0ce093f4854c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optim_vit_normal = optim.SGD(params=vit_normal.parameters(), lr=0.001, momentum=0.9, weight_decay=0.01)\n",
    "optim_vit_normal = optim.Adam(params=vit_normal.parameters(), lr=0.002, weight_decay=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ca449396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, batch 99: loss = 1.2524027824401855\n",
      "epoch 0, batch 299: loss = 1.3289605379104614\n",
      "test accuracy:  0.527899980545044 \n",
      "\n",
      "epoch 1, batch 99: loss = 1.2240197658538818\n",
      "epoch 1, batch 299: loss = 1.3136392831802368\n",
      "test accuracy:  0.5375999808311462 \n",
      "\n",
      "epoch 2, batch 99: loss = 1.2499678134918213\n",
      "epoch 2, batch 299: loss = 1.2821192741394043\n",
      "test accuracy:  0.5406999588012695 \n",
      "\n",
      "epoch 3, batch 99: loss = 1.2933807373046875\n",
      "epoch 3, batch 299: loss = 1.3026978969573975\n",
      "test accuracy:  0.49609997868537903 \n",
      "\n",
      "epoch 4, batch 99: loss = 1.3581798076629639\n",
      "epoch 4, batch 299: loss = 1.2873262166976929\n",
      "test accuracy:  0.5227999687194824 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "  vit_normal.train()\n",
    "  for i, data in enumerate(trainloader, 0):\n",
    "    inputs, labels = data\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    outputs = vit_normal(inputs)\n",
    "    optim_vit_normal.zero_grad()\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optim_vit_normal.step()\n",
    "\n",
    "    if i % 200 == 99:\n",
    "      print(f'epoch {epoch}, batch {i}: loss = {loss.item()}')\n",
    "\n",
    "  #eval model at end of epoch\n",
    "  vit_normal.eval()\n",
    "  acc = eval_model(vit_normal, testloader, device).item()\n",
    "  print('test accuracy: ', acc, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ae17c695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "ts = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "torch.save(vit_normal.state_dict(), f'./data/vit_normal_{ts}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ae44bf-a44b-4bdc-9922-9d4c805f1ce8",
   "metadata": {},
   "source": [
    "### Train ViT model using Student-Teacher with resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "e48189c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original dataset size:  50000\n",
      "new dataset size:  50000\n"
     ]
    }
   ],
   "source": [
    "#generate new \"soft\" trainset and dataloader with soft labels\n",
    "\n",
    "train_imgs = []\n",
    "soft_labels = []\n",
    "resnet18.eval()\n",
    "with torch.no_grad():\n",
    "  for i, data in enumerate(trainloader, 0):\n",
    "    inputs, labels = data\n",
    "    inputs = inputs.to(device)\n",
    "    outputs = resnet18(inputs)\n",
    "    soft_label = F.softmax(outputs, dim=-1)\n",
    "    \n",
    "    train_imgs.append(inputs.cpu())\n",
    "    soft_labels.append(soft_label.cpu())\n",
    "\n",
    "soft_trainset = torch.utils.data.TensorDataset(\n",
    "  torch.cat(train_imgs, dim=0),\n",
    "  torch.cat(soft_labels, dim=0),\n",
    ")\n",
    "soft_trainloader = torch.utils.data.DataLoader(\n",
    "  soft_trainset,\n",
    "  batch_size=TRAIN_BATCH_SIZE,\n",
    "  shuffle=True, \n",
    "  num_workers=2,\n",
    ")\n",
    "\n",
    "print('original dataset size: ', len(trainset))\n",
    "print('new dataset size: ', len(soft_trainset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1abcfbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
    "\n",
    "class MultiLabelDataset(Dataset):\n",
    "  def __init__(self, data, labels1, labels2, transform=None):\n",
    "    if len(data) != len(labels1) or len(data) != len(labels2):\n",
    "      raise ValueError(\"All input lists must have the same length.\")\n",
    "\n",
    "    self.data = data\n",
    "    self.labels1 = labels1\n",
    "    self.labels2 = labels2\n",
    "    self.transform = transform\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    sample = self.data[idx]\n",
    "    label1 = self.labels1[idx]\n",
    "    label2 = self.labels2[idx]\n",
    "\n",
    "    if self.transform:\n",
    "      sample = self.transform(sample)\n",
    "\n",
    "    return sample, label1, label2\n",
    "\n",
    "def get_soft_data(teacher, trainloader, batch_size, device):\n",
    "  datasets = []\n",
    "  imgs, hard_labels, soft_labels = [], [], []\n",
    "  teacher.eval()\n",
    "  with torch.no_grad():\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "      print(i)\n",
    "      inputs, labels = data\n",
    "      inputs = inputs.to(device)\n",
    "      outputs = teacher(inputs)\n",
    "      imgs.append(inputs.cpu())\n",
    "      hard_labels.append(labels)\n",
    "      soft_labels.append(outputs.cpu())\n",
    "\n",
    "  soft_trainset = MultiLabelDataset(\n",
    "    torch.concat(imgs, dim=0),\n",
    "    torch.concat(hard_labels, dim=0),\n",
    "    torch.concat(soft_labels, dim=0),\n",
    "  )\n",
    "  soft_trainloader = torch.utils.data.DataLoader(\n",
    "    soft_trainset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True, \n",
    "    num_workers=2,\n",
    "  )\n",
    "  return soft_trainset, soft_trainloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30415525",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model = get_teacher_model(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce9f392b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_cifar import get_cifar10_data\n",
    "trainset, testset, trainloader, testloader = get_cifar10_data(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feae5783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n"
     ]
    }
   ],
   "source": [
    "soft_trainset, soft_trainloader = get_soft_data(teacher_model, trainloader, 128, device)\n",
    "torch.save(soft_trainset, 'soft_trainset.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4ed7327",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/Desktop/programming/learning/ml/cv_selflearning/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/jacob/Desktop/programming/learning/ml/cv_selflearning/.venv/lib/python3.12/site-packages/outdated/__init__.py:36: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import parse_version\n"
     ]
    }
   ],
   "source": [
    "from load_cifar import MultiLabelDataset\n",
    "l_soft_ts = torch.load('soft_trainset.pt', weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2438efd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "da60777e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test that distributions were created properly \n",
    "data, dist = soft_trainset.__getitem__(0)\n",
    "data = data.to(device)\n",
    "out_test = resnet18(data.unsqueeze(0)).cpu()\n",
    "out_test = F.softmax(out_test, dim=-1)\n",
    "dist.allclose(out_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "a80bdabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of params: 1,998,602\n"
     ]
    }
   ],
   "source": [
    "vit_student = ViT(\n",
    "  embed_dim=128,\n",
    "  num_classes=10,\n",
    "  patch_size=4,\n",
    "  img_hw=(32, 32),\n",
    "  num_heads=8,\n",
    "  dropout=0.2,\n",
    ")\n",
    "vit_student.to(device)\n",
    "\n",
    "print(f'number of params: {sum([p.numel() for p in vit_normal.parameters()]):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "98ec541e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_vit_student = optim.Adam(params=vit_student.parameters(), lr=0.0001, weight_decay=0.001)\n",
    "student_criterion = nn.KLDivLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "b985beb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, batch 99: loss = 1.231581211090088\n",
      "epoch 0, batch 299: loss = 1.2767869234085083\n",
      "test accuracy:  0.257099986076355 \n",
      "\n",
      "epoch 1, batch 99: loss = 1.1059911251068115\n",
      "epoch 1, batch 299: loss = 1.174439787864685\n",
      "test accuracy:  0.24859999120235443 \n",
      "\n",
      "epoch 2, batch 99: loss = 1.3394922018051147\n",
      "epoch 2, batch 299: loss = 1.3345446586608887\n",
      "test accuracy:  0.2581999897956848 \n",
      "\n",
      "epoch 3, batch 99: loss = 1.2181655168533325\n",
      "epoch 3, batch 299: loss = 1.265332579612732\n",
      "test accuracy:  0.25519999861717224 \n",
      "\n",
      "epoch 4, batch 99: loss = 1.2227413654327393\n",
      "epoch 4, batch 299: loss = 1.2456021308898926\n",
      "test accuracy:  0.2563999891281128 \n",
      "\n",
      "epoch 5, batch 99: loss = 1.2425376176834106\n",
      "epoch 5, batch 299: loss = 1.1861951351165771\n",
      "test accuracy:  0.25529998540878296 \n",
      "\n",
      "epoch 6, batch 99: loss = 1.2538995742797852\n",
      "epoch 6, batch 299: loss = 1.3248958587646484\n",
      "test accuracy:  0.25439998507499695 \n",
      "\n",
      "epoch 7, batch 99: loss = 1.2701690196990967\n",
      "epoch 7, batch 299: loss = 1.3446366786956787\n",
      "test accuracy:  0.26579999923706055 \n",
      "\n",
      "epoch 8, batch 99: loss = 1.2764561176300049\n",
      "epoch 8, batch 299: loss = 1.2590651512145996\n",
      "test accuracy:  0.25429999828338623 \n",
      "\n",
      "epoch 9, batch 99: loss = 1.3117427825927734\n",
      "epoch 9, batch 299: loss = 1.2110028266906738\n",
      "test accuracy:  0.2500999867916107 \n",
      "\n",
      "epoch 10, batch 99: loss = 1.0974633693695068\n",
      "epoch 10, batch 299: loss = 1.2722499370574951\n",
      "test accuracy:  0.2507999837398529 \n",
      "\n",
      "epoch 11, batch 99: loss = 1.3070858716964722\n",
      "epoch 11, batch 299: loss = 1.1469030380249023\n",
      "test accuracy:  0.26569998264312744 \n",
      "\n",
      "epoch 12, batch 99: loss = 1.2785142660140991\n",
      "epoch 12, batch 299: loss = 1.1813921928405762\n",
      "test accuracy:  0.2533999979496002 \n",
      "\n",
      "epoch 13, batch 99: loss = 1.20209538936615\n",
      "epoch 13, batch 299: loss = 1.280557632446289\n",
      "test accuracy:  0.2587999999523163 \n",
      "\n",
      "epoch 14, batch 99: loss = 1.2045185565948486\n",
      "epoch 14, batch 299: loss = 1.2701961994171143\n",
      "test accuracy:  0.2595999836921692 \n",
      "\n",
      "epoch 15, batch 99: loss = 1.212428331375122\n",
      "epoch 15, batch 299: loss = 1.336601734161377\n",
      "test accuracy:  0.2459999918937683 \n",
      "\n",
      "epoch 16, batch 99: loss = 1.2207574844360352\n",
      "epoch 16, batch 299: loss = 1.3174198865890503\n",
      "test accuracy:  0.26440000534057617 \n",
      "\n",
      "epoch 17, batch 99: loss = 1.1590204238891602\n",
      "epoch 17, batch 299: loss = 1.3666284084320068\n",
      "test accuracy:  0.2549999952316284 \n",
      "\n",
      "epoch 18, batch 99: loss = 1.190549612045288\n",
      "epoch 18, batch 299: loss = 1.084616780281067\n",
      "test accuracy:  0.260699987411499 \n",
      "\n",
      "epoch 19, batch 99: loss = 1.2212803363800049\n",
      "epoch 19, batch 299: loss = 1.1926225423812866\n",
      "test accuracy:  0.27160000801086426 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "  resnet18.eval()\n",
    "  vit_student.train()\n",
    "  for i, data in enumerate(soft_trainloader, 0):\n",
    "    inputs, labels = data\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    outputs = vit_student(inputs)\n",
    "    outputs = F.log_softmax(outputs, dim=-1) #convert to log probabilities\n",
    "\n",
    "    optim_vit_student.zero_grad()\n",
    "    loss = F.kl_div(outputs, labels, reduction='batchmean')\n",
    "    #loss = student_criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optim_vit_student.step()\n",
    "\n",
    "    if i % 200 == 99:\n",
    "      print(f'epoch {epoch}, batch {i}: loss = {loss.item()}')\n",
    "\n",
    "  #eval model at end of epoch\n",
    "  vit_student.eval()\n",
    "  acc = eval_model(vit_student, testloader, device).item()\n",
    "  print('test accuracy: ', acc, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35140cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc55a628",
   "metadata": {},
   "source": [
    "### Testing TIMM ViT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b71fc6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "timm_vit = timm.create_model('vit_tiny_patch16_224', pretrained=False, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebf0b95e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5526346"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([p.numel() for p in timm_vit.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aaf322c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (patch_drop): Identity()\n",
       "  (norm_pre): Identity()\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "  (fc_norm): Identity()\n",
       "  (head_drop): Dropout(p=0.0, inplace=False)\n",
       "  (head): Linear(in_features=192, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timm_vit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
