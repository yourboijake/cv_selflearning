{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1c312c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea350868",
   "metadata": {},
   "outputs": [],
   "source": [
    "#demo using CIFAR10\n",
    "#create dataset with pytorch datset and dataloaders\n",
    "transform = torchvision.transforms.ToTensor()\n",
    "TRAIN_BATCH_SIZE = 128\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', \n",
    "    train=True,\n",
    "    download=True, \n",
    "    transform=transform,\n",
    ")\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, \n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True, \n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', \n",
    "    train=False,\n",
    "    download=True, \n",
    "    transform=transform,\n",
    ")\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, \n",
    "    batch_size=200,\n",
    "    shuffle=False, \n",
    "    num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42475925",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES=10\n",
    "IMG_PATCH_SIZE=2\n",
    "NUM_IMG_PATCHES=(32 * 32 * 3) / (IMG_PATCH_SIZE ** 2)\n",
    "IMG_PATCH_DIM=IMG_PATCH_SIZE ** 2 * 3\n",
    "\n",
    "#embeddings for both images and text\n",
    "img_embed = nn.Linear(in_features=IMG_PATCH_SIZE ** 2 * 3, out_features=1)\n",
    "text_embed = nn.Embedding(num_embeddings=NUM_CLASSES, embedding_dim=64)\n",
    "\n",
    "#mapping from embedding to shared embedding space\n",
    "img_embed_map = nn.Linear(in_features=256, out_features=32)\n",
    "text_embed_map = nn.Linear(in_features=64, out_features=32)\n",
    "\n",
    "#learnable temperature parameter\n",
    "tau = nn.Parameter(torch.randn(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c067346e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 3, 32, 32]), torch.Size([128]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = next(iter(trainloader))\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e02b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward pass\n",
    "\n",
    "img_patches = F.unfold(X, kernel_size=2, stride=2).transpose(-2, -1)\n",
    "img_e = img_embed(img_patches).squeeze(-1)\n",
    "labels_e = text_embed(y)\n",
    "\n",
    "img_e = img_embed_map(img_e)\n",
    "labels_e = text_embed_map(labels_e)\n",
    "\n",
    "#l2 norm\n",
    "img_e = F.normalize(img_e, dim=-1)\n",
    "labels_e = F.normalize(labels_e, dim=-1)\n",
    "\n",
    "#cos sim\n",
    "logits = img_e @ labels_e.transpose(-2, -1) * torch.exp(tau)\n",
    "\n",
    "#losses\n",
    "batch_size = logits.shape[0]\n",
    "loss_i = F.cross_entropy(logits, torch.arange(batch_size))\n",
    "loss_t = F.cross_entropy(logits, torch.arange(batch_size))\n",
    "loss = (loss_i + loss_t) / 2\n",
    "\n",
    "loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
