{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1053ff0b-946b-462f-b57f-96035c39419b",
   "metadata": {},
   "source": [
    "### Implementation of AlexNet\n",
    "\n",
    "- Architecture: 8 layers, (5 conv then 3 fully connected)\n",
    "- ReLU activations (found to be superior to tanh)\n",
    "- local response normalization:\n",
    "    - normalize against activation of same pixel region from adjacent kernels in kernel list. A bit hacky?\n",
    "    - Superseded by batch norm, which is used more commonly now\n",
    "- overlapping pooling: slight performance advantages and less overfitting with overlapping pooling\n",
    "    - traditional pooling: stride = pooling window side length\n",
    "    - overlapping pooling: stride < pooling window side length\n",
    "    - \"dilated pooling\"? (not a given term, just my term), stride > pooling window side length\n",
    "- 0.5 dropout for regularization\n",
    "\n",
    "Training procedure\n",
    "- batch size 128\n",
    "- momentum of 0.9\n",
    "- L2 reg with lambda of 0.0005\n",
    "- weight init with zero-mean gaussian with std 0.01, all biases init to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3aa9527-cd85-460d-96cc-443956081a2e",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd36bda7-28cb-42b7-b30f-373f77f46926",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "\n",
    "if torch.xpu.is_available():\n",
    "    device = torch.device('xpu')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71dca121-62ce-437f-9228-5b24e8c8c093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "cifar_data_1 = unpickle('cifar-10-python/data_batch_1')\n",
    "cifar_data_2 = unpickle('cifar-10-python/data_batch_2')\n",
    "cifar_data_3 = unpickle('cifar-10-python/data_batch_3')\n",
    "cifar_data_4 = unpickle('cifar-10-python/data_batch_4')\n",
    "cifar_data_5 = unpickle('cifar-10-python/data_batch_5')\n",
    "\n",
    "Xtr = np.concatenate([cifar_data_1[b'data'], cifar_data_2[b'data'], cifar_data_3[b'data'], cifar_data_4[b'data']])\n",
    "ytr = np.concatenate([cifar_data_1[b'labels'], cifar_data_2[b'labels'], cifar_data_3[b'labels'], cifar_data_4[b'labels']])\n",
    "Xtst = cifar_data_5[b'data']\n",
    "ytst = cifar_data_5[b'labels']\n",
    "\n",
    "Xtr = torch.tensor(Xtr.reshape(-1, 3, 32, 32), dtype=torch.float32) / 255.0 #normalize between 0 and 1\n",
    "Xtst = torch.tensor(Xtst.reshape(-1, 3, 32, 32), dtype=torch.float32) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80b20871-766e-4536-8b45-287d879bd4a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4915, 0.4821, 0.4462])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtr.mean(dim=(0, 2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3d017a-2043-4fd2-a34b-16ef5f7cecd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.transpose(np.reshape(Xtr[22],(3, 32,32)), (1,2,0))\n",
    "print(ytr[4])\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "222bb746-d13a-44b3-83ca-59d3e2508fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create pytorch dataset and dataloader objects\n",
    "class Cifar10Dataset(Dataset):\n",
    "    def __init__(self, input_tensor, labels, transform=None):\n",
    "        if input_tensor.shape[0] != len(labels):\n",
    "            raise ValueError(\"Input tensor and labels must have the same number of samples.\")\n",
    "        self.input_tensor = input_tensor\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.input_tensor.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample_input = self.input_tensor[idx]\n",
    "        sample_label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            sample_input = self.transform(sample_input)\n",
    "\n",
    "        return sample_input, sample_label\n",
    "\n",
    "train_mean = Xtr.mean(dim=(0, 2, 3))\n",
    "train_std = Xtr.mean(dim=(0, 2, 3))\n",
    "tr_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.RandomRotation(10),\n",
    "    torchvision.transforms.Normalize(mean=train_mean, std=train_std),\n",
    "])\n",
    "tst_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Normalize(mean=train_mean, std=train_std),\n",
    "])\n",
    "cifar10_tr = Cifar10Dataset(Xtr, ytr, transform=tr_transform)\n",
    "cifar10_tst = Cifar10Dataset(Xtst, ytst, transform=tst_transform)\n",
    "\n",
    "# Create DataLoaders for training and validation\n",
    "tr_dataloader = DataLoader(cifar10_tr, batch_size=32, shuffle=True, num_workers=0)\n",
    "tst_dataloader = DataLoader(cifar10_tst, batch_size=32, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ceddc48-abe7-4b43-87bb-be99dcbe092b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters:  3673856\n"
     ]
    }
   ],
   "source": [
    "#implementing alexnet, with a few modifications\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, p=0.5):\n",
    "        super(AlexNet, self).__init__()\n",
    "        n_kernels = 32\n",
    "        self.l1 = nn.Sequential(\n",
    "            nn.Conv2d(3, n_kernels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(n_kernels),\n",
    "            nn.Dropout(p),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.l2 = nn.Sequential(\n",
    "            nn.Conv2d(n_kernels, n_kernels*2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(n_kernels*2),\n",
    "            nn.Dropout(p),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        n_kernels *= 2\n",
    "        self.l3 = nn.Sequential(\n",
    "            nn.Conv2d(n_kernels, n_kernels*2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(n_kernels*2),\n",
    "            nn.Dropout(p),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        n_kernels *= 2\n",
    "        self.l4 = nn.Sequential(\n",
    "            nn.Conv2d(n_kernels, n_kernels*2, kernel_size=3, padding=0),\n",
    "            nn.BatchNorm2d(n_kernels*2),\n",
    "            nn.Dropout(p),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        n_kernels *= 2\n",
    "        self.l5 = nn.Sequential(\n",
    "            nn.Conv2d(n_kernels, n_kernels*2, kernel_size=3, padding=0),\n",
    "            nn.BatchNorm2d(n_kernels*2),\n",
    "            nn.Dropout(p),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.l6 = nn.Sequential(\n",
    "            nn.Conv2d(n_kernels*2, n_kernels*2, kernel_size=2, padding=0),\n",
    "            nn.Dropout(p),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        n_kernels *= 2\n",
    "        self.l7 = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(p),\n",
    "            nn.Linear(n_kernels * 2 * 2, n_kernels, bias=True),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.l8 = nn.Linear(n_kernels, 10, bias=False)\n",
    "\n",
    "    def forward(self, X):\n",
    "        out = self.l1(X)\n",
    "        out = self.l2(out)\n",
    "        out = self.l3(out)\n",
    "        out = self.l4(out)\n",
    "        out = self.l5(out)\n",
    "        out = self.l6(out)\n",
    "        out = self.l7(out)\n",
    "        out = self.l8(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "model = AlexNet(p=0.6)\n",
    "model.to(device) #move to xpu\n",
    "\n",
    "print('number of parameters: ', sum([p.numel() for p in model.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "003453be-ae34-4fa9-a03c-1f6f991a44db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparams\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 64\n",
    "learning_rate = 0.005\n",
    "l2_lambda = 0.05\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam(params=model.parameters(), lr=learning_rate, weight_decay=l2_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3ceafff-ff8a-4ada-8762-ba34cc6ab3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training\n",
      "Epoch 0: \n",
      " train loss: 2.129885676383972, train_acc: 17.0475 \n",
      " test_loss: 2.2406866771698, test_acc: 19.15\n",
      "Epoch 1: \n",
      " train loss: 2.04389357881546, train_acc: 17.6925 \n",
      " test_loss: 2.2220453674316407, test_acc: 15.63\n",
      "Epoch 2: \n",
      " train loss: 2.0500806181907656, train_acc: 17.515 \n",
      " test_loss: 2.2444885353088377, test_acc: 18.61\n",
      "Epoch 3: \n",
      " train loss: 2.0393212312698363, train_acc: 17.8325 \n",
      " test_loss: 2.2434445457458496, test_acc: 12.09\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m train_correct = \u001b[32m0\u001b[39m\n\u001b[32m     11\u001b[39m model.train()\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtr_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#move to XPU\u001b[39;49;00m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mxpu\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mxpu\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/programming/learning/ml/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/programming/learning/ml/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/programming/learning/ml/venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mCifar10Dataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     15\u001b[39m sample_label = \u001b[38;5;28mself\u001b[39m.labels[idx]\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform:\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     sample_input = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m sample_input, sample_label\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/programming/learning/ml/venv/lib/python3.12/site-packages/torchvision/transforms/transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/programming/learning/ml/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/programming/learning/ml/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/programming/learning/ml/venv/lib/python3.12/site-packages/torchvision/transforms/transforms.py:277\u001b[39m, in \u001b[36mNormalize.forward\u001b[39m\u001b[34m(self, tensor)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: Tensor) -> Tensor:\n\u001b[32m    270\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    271\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    272\u001b[39m \u001b[33;03m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    275\u001b[39m \u001b[33;03m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[32m    276\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/programming/learning/ml/venv/lib/python3.12/site-packages/torchvision/transforms/functional.py:350\u001b[39m, in \u001b[36mnormalize\u001b[39m\u001b[34m(tensor, mean, std, inplace)\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, torch.Tensor):\n\u001b[32m    348\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mimg should be Tensor Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensor)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/programming/learning/ml/venv/lib/python3.12/site-packages/torchvision/transforms/_functional_tensor.py:925\u001b[39m, in \u001b[36mnormalize\u001b[39m\u001b[34m(tensor, mean, std, inplace)\u001b[39m\n\u001b[32m    923\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mstd evaluated to zero after conversion to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, leading to division by zero.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    924\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mean.ndim == \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m     mean = \u001b[43mmean\u001b[49m\u001b[43m.\u001b[49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m std.ndim == \u001b[32m1\u001b[39m:\n\u001b[32m    927\u001b[39m     std = std.view(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#training loop\n",
    "\n",
    "train_loss = 0.0\n",
    "train_correct = 0\n",
    "losses = []\n",
    "\n",
    "print('starting training')\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    model.train()\n",
    "\n",
    "    for i, (data, target) in enumerate(tr_dataloader):\n",
    "        #move to XPU\n",
    "        data = data.to(dtype=torch.float32, device='xpu')\n",
    "        target = target.to(device='xpu')\n",
    "\n",
    "        #run forward and backprop\n",
    "        optim.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "        train_loss += loss.item() * data.size(0)\n",
    "        _, pred = torch.max(output, 1)\n",
    "        train_correct += (pred == target).sum().item()\n",
    "    \n",
    "    train_loss /= len(tr_dataloader.dataset)\n",
    "    train_acc = 100.0 * train_correct / len(tr_dataloader.dataset)\n",
    "    \n",
    "    test_loss = 0.0\n",
    "    test_correct = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in tst_dataloader:\n",
    "            data = data.to(dtype=torch.float32, device='xpu')\n",
    "            target = target.to(device='xpu')\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "    \n",
    "            test_loss += loss.item() * data.size(0)\n",
    "            _, pred = torch.max(output, 1)\n",
    "            test_correct += (pred == target).sum().item()\n",
    "    \n",
    "    test_loss /= len(tst_dataloader.dataset)\n",
    "    test_acc = 100.0 * test_correct / len(tst_dataloader.dataset)\n",
    "\n",
    "    print(f'Epoch {epoch}: \\n train loss: {train_loss}, train_acc: {train_acc} \\n test_loss: {test_loss}, test_acc: {test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7df3df-52ec-4bce-93a9-d7078818ae7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(losses)), losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d74fa50-cb70-47c8-8175-74b5cd0301f7",
   "metadata": {},
   "source": [
    "### Take 2: Less Pooling and Dropouts\n",
    "- modeled after: https://nvsyashwanth.github.io/machinelearningmaster/cifar-10/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f4fabc-271b-4efb-a6c8-f8fbfc65e250",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed5a5c4e-efe2-40b6-b1c5-bbac81154da7",
   "metadata": {},
   "source": [
    "#### Experiment tracking\n",
    "- hugely overfitting: final train acc of 25.45, test acc of 16.99. Also, still very low acc overall. Trying: increasing dropout rate, increasing l2_lambda. Also reducing lr from 0.005 to 0.003, due to non-convergence. Model seems to be too small, also, given the low acc, but how to prevent overfitting?\n",
    "- still hugely overfitting, increasing l2_lambda to 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "544476be-960f-4c23-b020-54df1fff0c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters:  5851338\n"
     ]
    }
   ],
   "source": [
    "#https://www.kaggle.com/code/shadabhussain/cifar-10-cnn-using-pytorch\n",
    "\n",
    "class KaggleCifar10(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n",
    "    \n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 128 x 8 x 8\n",
    "    \n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 256 x 4 x 4\n",
    "    \n",
    "            nn.Flatten(), \n",
    "            nn.Linear(256*4*4, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        return self.network(xb)\n",
    "\n",
    "knet = KaggleCifar10()\n",
    "knet.to(device)\n",
    "print('number of parameters: ', sum([p.numel() for p in knet.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc7610b4-5018-4fd7-912e-e9cc1a7b5bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparams\n",
    "num_classes = 10\n",
    "num_epochs = 2\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "l2_lambda = 0.05\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam(params=knet.parameters(), lr=learning_rate, weight_decay=l2_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc307252-96fc-49bf-95cc-c42d030cd649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training\n",
      "Epoch 0: \n",
      " train loss: 2.3028072072982786, train_acc: 9.8775 \n",
      " test_loss: 2.3026850143432616, test_acc: 9.97\n",
      "Epoch 1: \n",
      " train loss: 2.3027570476531984, train_acc: 9.9175 \n",
      " test_loss: 2.3026883331298826, test_acc: 10.03\n"
     ]
    }
   ],
   "source": [
    "#training loop\n",
    "\n",
    "train_loss = 0.0\n",
    "train_correct = 0\n",
    "losses = []\n",
    "\n",
    "print('starting training')\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    knet.train()\n",
    "\n",
    "    for i, (data, target) in enumerate(tr_dataloader):\n",
    "        #move to XPU\n",
    "        data = data.to(dtype=torch.float32, device='xpu')\n",
    "        target = target.to(device='xpu')\n",
    "\n",
    "        #run forward and backprop\n",
    "        optim.zero_grad()\n",
    "        output = knet(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "        train_loss += loss.item() * data.size(0)\n",
    "        _, pred = torch.max(output, 1)\n",
    "        train_correct += (pred == target).sum().item()\n",
    "    \n",
    "    train_loss /= len(tr_dataloader.dataset)\n",
    "    train_acc = 100.0 * train_correct / len(tr_dataloader.dataset)\n",
    "    \n",
    "    test_loss = 0.0\n",
    "    test_correct = 0\n",
    "    knet.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in tst_dataloader:\n",
    "            data = data.to(dtype=torch.float32, device='xpu')\n",
    "            target = target.to(device='xpu')\n",
    "            output = knet(data)\n",
    "            loss = criterion(output, target)\n",
    "    \n",
    "            test_loss += loss.item() * data.size(0)\n",
    "            _, pred = torch.max(output, 1)\n",
    "            test_correct += (pred == target).sum().item()\n",
    "    \n",
    "    test_loss /= len(tst_dataloader.dataset)\n",
    "    test_acc = 100.0 * test_correct / len(tst_dataloader.dataset)\n",
    "\n",
    "    print(f'Epoch {epoch}: \\n train loss: {train_loss}, train_acc: {train_acc} \\n test_loss: {test_loss}, test_acc: {test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd0db5d-c0d5-41a0-8291-285f322c4d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(losses)), losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
