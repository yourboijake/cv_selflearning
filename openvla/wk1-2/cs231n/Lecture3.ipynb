{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db7154b1-6eae-45ac-98ac-e61e7a2acfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9743303d-af24-4f3e-a4df-47f735836929",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_data_1 = unpickle('cifar-10-python/data_batch_1')\n",
    "cifar_data_2 = unpickle('cifar-10-python/data_batch_2')\n",
    "cifar_data_3 = unpickle('cifar-10-python/data_batch_3')\n",
    "cifar_data_4 = unpickle('cifar-10-python/data_batch_4')\n",
    "cifar_data_5 = unpickle('cifar-10-python/data_batch_5')\n",
    "\n",
    "Xtr = np.concatenate([cifar_data_1[b'data'], cifar_data_2[b'data'], cifar_data_3[b'data'], cifar_data_4[b'data']])\n",
    "ytr = np.concatenate([cifar_data_1[b'labels'], cifar_data_2[b'labels'], cifar_data_3[b'labels'], cifar_data_4[b'labels']])\n",
    "Xtst = cifar_data_5[b'data']\n",
    "ytst = cifar_data_5[b'labels']\n",
    "\n",
    "Xtr_norm = (Xtr - Xtr.mean(axis=0)) / Xtr.var(axis=0)\n",
    "Xtst_norm = (Xtst - Xtst.mean(axis=0)) / Xtst.var(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc011b2f-3ae6-4f2e-be77-45406acdc687",
   "metadata": {},
   "source": [
    "#### Optimization: Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76b4f36e-3723-4f61-a906-5c8b56b7aa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implement linear classifier\n",
    "class LinearClassifier:\n",
    "    def __init__(self, D, K):\n",
    "        self.W = np.random.normal(size=(D, K)) / D ** 0.5\n",
    "        self.b = np.random.normal(size=(K, )) / K ** 0.5\n",
    "\n",
    "    def forward(self, X):\n",
    "        return X @ self.W  + self.b\n",
    "   \n",
    "    def loss(self, y_pred, y_true, delta, lmbda):\n",
    "        '''\n",
    "        y_pred = matrix of shape (N x K)\n",
    "        y_true = vector of dim N\n",
    "        delta = scalar\n",
    "        lmbda = scalar\n",
    "        '''\n",
    "        yt_pred = y_pred[np.arange(len(y_true)), y_true]\n",
    "        \n",
    "        #use loss mask to remove the correct y trues from loss calculation\n",
    "        yt_loss_mask = np.ones(y_pred.shape)\n",
    "        yt_loss_mask[np.arange(yt_loss_mask.shape[0]), y_true] = 0\n",
    "\n",
    "        dist = (y_pred.T - yt_pred + delta).T * yt_loss_mask\n",
    "        l2_reg = lmbda * (np.sum(self.W ** 2) + np.sum(self.b ** 2))\n",
    "        loss = np.sum(np.clip(dist, a_min=0, a_max=np.inf)) + l2_reg\n",
    "\n",
    "        return loss, dist\n",
    "\n",
    "    def msvm_grad(self, X, dist, lmbda):\n",
    "        '''\n",
    "        args:\n",
    "            X = input data (N x D), where N = training set/batch size, D = data dimensionality\n",
    "            dist = distance matrix (N x K) computed using multiclass SVM loss, where K = number of output categories\n",
    "        algo:\n",
    "            matmul input data transpose against distance matrix, yields gradient matrix (D x K)\n",
    "            L2 regularization loss also included\n",
    "        '''\n",
    "        W_grad = X.T @ dist + 2 * lmbda * self.W\n",
    "        b_grad = dist.mean(axis=0) + 2 * lmbda * self.b\n",
    "        return W_grad, b_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5998fb94-49fc-40a5-94aa-b8a4cca4d257",
   "metadata": {},
   "source": [
    "#### Training the Linear Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cdb00c4d-6a02-4caa-a923-d17db400c7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc = LinearClassifier(3072, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "133802d8-a642-4a40-8874-4275527ed129",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper params\n",
    "DIST = 1.0\n",
    "LMBDA = 0.5\n",
    "BATCH_SIZE = 64\n",
    "N_ITERS = 500\n",
    "\n",
    "#randomly generate indexes for batches\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "#run training loop\n",
    "losses, W_grads, b_grads, Ws, bs = [], [], [], [], []\n",
    "for _ in range(N_ITERS):\n",
    "    #get batch data\n",
    "    batch_idx = rng.choice(Xtr_norm.shape[0], size=BATCH_SIZE, replace=False)\n",
    "    X_batch = Xtr_norm[batch_idx]\n",
    "    y_batch = ytr[batch_idx]\n",
    "\n",
    "    #forward pass\n",
    "    y_pred = lc.forward(X_batch)\n",
    "\n",
    "    #backward pass\n",
    "    loss, dist = lc.loss(y_pred, y_batch, DIST, LMBDA)\n",
    "    W_grad, b_grad = lc.msvm_grad(X_batch, dist, LMBDA)\n",
    "    lr = 0.005 #learning rate\n",
    "    lc.W = -lr * W_grad + lc.W\n",
    "    lc.b = -lr * b_grad + lc.b\n",
    "\n",
    "    #logging changes\n",
    "    losses.append(loss)\n",
    "    W_grads.append(W_grad)\n",
    "    b_grads.append(b_grad)\n",
    "    Ws.append(lc.W)\n",
    "    bs.append(lc.b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
